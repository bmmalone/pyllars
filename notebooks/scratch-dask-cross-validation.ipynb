{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting model hyperparameters by cross-validation\n",
    "\n",
    "The overview is that we will split the dataset into `args.num_folds` distinct partitions (\"folds\"). (This example description will use `args.num_folds=10`.) Eight of the folds will be used for **training**, one will be used for **validation**, and one for **testing**. This is performed for each set of hyperparameters. Finally, that process is repeated using each fold as the validation and test set. In this work, when fold `i` is the validation set, then fold `i+1%10` is the test set; the other folds are taken as the training set. Other strategies can be used, but this results in exactly a single prediction for each data record in each of the validation and test set for each set of hyperparameters.\n",
    "\n",
    "The rough purpose of each of the dataset types is as follows.\n",
    "\n",
    "* **training**. given the hyperparameters, set the model parameters.\n",
    "* **validation**. select the hyperparameters which perform the best on an unseen dataset.\n",
    "* **testing**. evaluate the selected hyperparameters on a different unseen dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal\n",
    "\n",
    "This notebook demonstrates the necessary steps to distribute training across a cluster using `dask`,  select hyperparameters using a validation set, collect the test set predictions, and evaluate the model performance using the `pyllars` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO     : [dask_utils]: starting local dask cluster\n",
      "DEBUG    : Using selector: EpollSelector\n",
      "DEBUG    : Using selector: EpollSelector\n",
      "DEBUG    : Using selector: EpollSelector\n",
      "DEBUG    : Using selector: EpollSelector\n",
      "DEBUG    : Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# initialize a logger for ipython\n",
    "import pyllars.logging_utils as logging_utils\n",
    "logger = logging_utils.get_ipython_logger()\n",
    "\n",
    "# create an argparse namespace to hold parameters\n",
    "from argparse import Namespace\n",
    "args = Namespace()\n",
    "\n",
    "# create (or connect to) a dask cluster\n",
    "import pyllars.dask_utils as dask_utils\n",
    "cluster_location = \"LOCAL\"\n",
    "\n",
    "cluster_restart=False\n",
    "dask_utils.add_dask_values_to_args(\n",
    "    args,\n",
    "    cluster_location=cluster_location,\n",
    "    num_procs=3,\n",
    "    num_threads_per_proc=1\n",
    ")\n",
    "\n",
    "dask_client, cluster = dask_utils.connect(args)\n",
    "\n",
    "# machine learning imports\n",
    "import sklearn.datasets\n",
    "import sklearn.model_selection\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "\n",
    "import xgboost\n",
    "\n",
    "# other tools and helpers\n",
    "import itertools\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyllars.ml_utils as ml_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide \"command line\" arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.random_state = 8675309 # several steps require a seed. we will use the same one to avoid unexpected results.\n",
    "args.num_folds = 10 # use 10-fold cross-validation\n",
    "\n",
    "args.evaluation_metric = 'mean_squared_error'\n",
    "args.selection_strategy = np.argmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# standard pydata imports\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import seaborn as sns; sns.set(style='white', color_codes=True)\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: The evaluation always uses `predict`. Currently, there is not a way to tell it to use `predict_proba` (or anything else)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Container, Dict, Iterable, List, NamedTuple, Optional, Set, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function *is not* in ml_utils\n",
    "# we can do whatever we want here\n",
    "def evaluate_hyperparameters_helper(\n",
    "        hv:List,\n",
    "        args:Namespace,\n",
    "        estimator_template:sklearn.base.BaseEstimator,\n",
    "        data:pd.DataFrame,\n",
    "        collect_metrics:Callable,\n",
    "        train_folds:Optional[Any]=None,\n",
    "        split_field:str='fold',\n",
    "        target_field:str='target',\n",
    "        target_transform:Optional[Callable]=None,\n",
    "        target_inverse_transform:Optional[Callable]=None,\n",
    "        collect_metrics_kwargs:Optional[Dict]=None,\n",
    "        fields_to_ignore:Optional[Container[str]]=None) -> NamedTuple:\n",
    "    \n",
    "    # these come from our iterator\n",
    "    hyperparameters = hv[0]\n",
    "    validation_folds = hv[1]\n",
    "    \n",
    "    # we know we are doing 10-fold cv\n",
    "    test_folds = (validation_folds + 1) % args.num_folds\n",
    "    \n",
    "    res = ml_utils.evaluate_hyperparameters(\n",
    "        estimator_template=estimator_template,\n",
    "        hyperparameters=hyperparameters,\n",
    "        validation_folds=validation_folds,\n",
    "        test_folds=test_folds,\n",
    "        data=data,\n",
    "        collect_metrics=collect_metrics,\n",
    "        train_folds=train_folds,\n",
    "        split_field=split_field,\n",
    "        target_field=target_field,\n",
    "        target_transform=target_transform,\n",
    "        target_inverse_transform=target_inverse_transform,\n",
    "        collect_metrics_kwargs=collect_metrics_kwargs,\n",
    "        fields_to_ignore=fields_to_ignore\n",
    "    )\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a small regression dataset\n",
    "\n",
    "We will use the `boston` dataset made available in `sklearn`. All of its features are numeric, and the target is also numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    data = sklearn.datasets.load_boston()\n",
    "    df = pd.DataFrame(data['data'], columns=data['feature_names'])\n",
    "    df['target'] = data['target']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset()\n",
    "\n",
    "###\n",
    "# Determine the fold of row.\n",
    "#\n",
    "# N.B. This could be performed once in an \"offline\" step\n",
    "#    and stored as another column in the dataframe\n",
    "###\n",
    "folds = ml_utils.get_cv_folds(\n",
    "    df['target'],\n",
    "    num_splits=args.num_folds,\n",
    "    use_stratified=False, # stratified does not work for regression\n",
    "    shuffle=True,\n",
    "    random_state=args.random_state # ensure we always shuffle the same way\n",
    ")\n",
    "\n",
    "# add a column to indicate the fold of each row\n",
    "df['fold'] = folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>target</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  target  fold  \n",
       "0     15.3  396.90   4.98    24.0     3  \n",
       "1     17.8  396.90   9.14    21.6     9  \n",
       "2     17.8  392.83   4.03    34.7     5  \n",
       "3     18.7  394.63   2.94    33.4     4  \n",
       "4     18.7  396.90   5.33    36.2     6  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create our estimator and hyperparameter grid\n",
    "\n",
    "We will use a simple scaling followed by extreme gradient boosting for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could also include things like PCA here. However, one\n",
    "# advantage of trees (and forests) is that we can assign\n",
    "# importances to features; if we use dimensionality reduction\n",
    "# or other techniques which significantly change the\n",
    "# interpretation of the features, though, we largely lose\n",
    "# that advantage.\n",
    "estimator_template = sklearn.pipeline.Pipeline([\n",
    "    ('scaler',sklearn.preprocessing.StandardScaler()),\n",
    "    ('xgb', xgboost.XGBRegressor())\n",
    "])\n",
    "\n",
    "# In practice, this is a small hyperparameter grid; xgboost has\n",
    "# many more hyperparameters that can be worth investigating.\n",
    "#\n",
    "# A quick overview of some of the most important hyperparameters and\n",
    "# their interpretation is available here:\n",
    "#   https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/ \n",
    "hyperparam_grid = sklearn.model_selection.ParameterGrid({\n",
    "    'xgb__n_estimators': [50, 100, 500], \n",
    "    'xgb__learning_rate': [.01,0.1],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an iterator over folds and hyperparameter configurations\n",
    "\n",
    "As described in the introduction, we will test each set of hyperparameters in our grid on each fold (while training on eight of the other folds). For selecting our final set of hyperparameters, we will use performance on the validation fold. (More details are given on this procedure below.)\n",
    "\n",
    "Concretely, we will accomplish this by iterating over the cross-product of the sets of hyperparameters and validation set folds. (As described, given the validation fold index, we can determine the training and testing folds.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for simplicity, just create lists of everything\n",
    "\n",
    "# In principle, a lazy generator or something more fancy could\n",
    "# be used; in practice, this is usually not necessary unless\n",
    "# the hyperparameter grid contains very large objects.\n",
    "hyperparam_grid = list(hyperparam_grid)\n",
    "folds = list(range(args.num_folds))\n",
    "\n",
    "# an iterator over (hyperparameter, validation_fold) tuples\n",
    "hp_fold_it = itertools.product(hyperparam_grid, folds)\n",
    "hp_fold_it = list(hp_fold_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>target</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  target  fold  \n",
       "0     15.3  396.90   4.98    24.0     3  \n",
       "1     17.8  396.90   9.14    21.6     9  \n",
       "2     17.8  392.83   4.03    34.7     5  \n",
       "3     18.7  394.63   2.94    33.4     4  \n",
       "4     18.7  396.90   5.33    36.2     6  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'explained_variance': 0.8744051444128544,\n",
       " 'mean_absolute_error': 1.7798925885967178,\n",
       " 'mean_squared_error': 7.453449727978154,\n",
       " 'median_absolute_error': 1.0303699493408196,\n",
       " 'r2': 0.8743925298651639}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters, validation_fold = hp_fold_it[51]\n",
    "test_fold = (validation_fold + 1) % 10\n",
    "\n",
    "res = ml_utils.evaluate_hyperparameters(\n",
    "    estimator_template=estimator_template,\n",
    "    hyperparameters=hyperparameters,\n",
    "    validation_folds=validation_fold,\n",
    "    test_folds=test_fold,\n",
    "    data=df,\n",
    "    split_field='fold',\n",
    "    target_field='target',\n",
    "    collect_metrics=ml_utils.collect_regression_metrics,\n",
    "    target_transform=np.log1p,\n",
    "    target_inverse_transform=np.expm1\n",
    ")\n",
    "\n",
    "res.metrics_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'explained_variance': 0.8744051444128544,\n",
       " 'mean_absolute_error': 1.7798925885967178,\n",
       " 'mean_squared_error': 7.453449727978154,\n",
       " 'median_absolute_error': 1.0303699493408196,\n",
       " 'r2': 0.8743925298651639}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = evaluate_hyperparameters_helper(\n",
    "    hp_fold_it[51],\n",
    "    args=args,\n",
    "    estimator_template=estimator_template,\n",
    "    data=df,\n",
    "    split_field='fold',\n",
    "    target_field='target',\n",
    "    collect_metrics=ml_utils.collect_regression_metrics,\n",
    "    target_transform=np.log1p,\n",
    "    target_inverse_transform=np.expm1\n",
    ")\n",
    "\n",
    "res.metrics_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"xgb__learning_rate\": 0.1, \"xgb__n_estimators\": 500}'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.hyperparameters_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG    : [dask_utils.apply_iter] submitting jobs to cluster\n",
      "100%|██████████| 60/60 [00:05<00:00, 10.38it/s]\n"
     ]
    }
   ],
   "source": [
    "f_res = dask_utils.apply_iter(\n",
    "    hp_fold_it,\n",
    "    dask_client,\n",
    "    evaluate_hyperparameters_helper,\n",
    "    args=args,\n",
    "    estimator_template=estimator_template,\n",
    "    data=df,\n",
    "    split_field='fold',\n",
    "    target_field='target',\n",
    "    collect_metrics=ml_utils.collect_regression_metrics,\n",
    "    target_transform=np.log1p,\n",
    "    target_inverse_transform=np.expm1,\n",
    "    return_futures=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'finished': 60})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dask_utils.check_status(f_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res = dask_utils.collect_results(f_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'explained_variance': 0.8744051444128544,\n",
       " 'mean_absolute_error': 1.7798925885967178,\n",
       " 'mean_squared_error': 7.453449727978154,\n",
       " 'median_absolute_error': 1.0303699493408196,\n",
       " 'r2': 0.8743925298651639}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_res[51].metrics_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_res(res):\n",
    "    ret_val = {\n",
    "        'validation_{}'.format(k): v\n",
    "            for k,v in res.metrics_val.items()\n",
    "    }\n",
    "    \n",
    "    ret_test = {\n",
    "        'test_{}'.format(k): v\n",
    "            for k,v in res.metrics_test.items()\n",
    "    }\n",
    "    \n",
    "    ret = ret_val\n",
    "    ret.update(ret_test)\n",
    "    \n",
    "    hp_string = json.dumps(res.hyperparameters)\n",
    "    ret['hyperparameters_str'] = hp_string\n",
    "    \n",
    "    ret['hyperparameters'] = res.hyperparameters\n",
    "    ret['validation_fold'] = res.fold_val\n",
    "    ret['test_fold'] = res.fold_test\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Create the results data frame\n",
    "###\n",
    "results = [\n",
    "    _get_res(res) for res in all_res\n",
    "]\n",
    "\n",
    "df_results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Based on the performance on the validation set, select\n",
    "# the best hyperparameters.\n",
    "###\n",
    "hp_groups = df_results.groupby('hyperparameters_str')\n",
    "\n",
    "validation_evaluation_metric = \"validation_{}\".format(args.evaluation_metric)\n",
    "test_evaluation_metric = \"test_{}\".format(args.evaluation_metric)\n",
    "\n",
    "val_performance = hp_groups[validation_evaluation_metric].mean()\n",
    "\n",
    "# now, select the best\n",
    "val_best = args.selection_strategy(val_performance)\n",
    "val_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_val_best = (df_results['hyperparameters_str'] == val_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hyperparameters</th>\n",
       "      <th>hyperparameters_str</th>\n",
       "      <th>test_explained_variance</th>\n",
       "      <th>test_fold</th>\n",
       "      <th>test_mean_absolute_error</th>\n",
       "      <th>test_mean_squared_error</th>\n",
       "      <th>test_median_absolute_error</th>\n",
       "      <th>test_r2</th>\n",
       "      <th>validation_explained_variance</th>\n",
       "      <th>validation_fold</th>\n",
       "      <th>validation_mean_absolute_error</th>\n",
       "      <th>validation_mean_squared_error</th>\n",
       "      <th>validation_median_absolute_error</th>\n",
       "      <th>validation_r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>{'xgb__learning_rate': 0.1, 'xgb__n_estimators...</td>\n",
       "      <td>{\"xgb__learning_rate\": 0.1, \"xgb__n_estimators...</td>\n",
       "      <td>0.874402</td>\n",
       "      <td>1</td>\n",
       "      <td>1.753658</td>\n",
       "      <td>7.543395</td>\n",
       "      <td>1.297095</td>\n",
       "      <td>0.872877</td>\n",
       "      <td>0.912643</td>\n",
       "      <td>0</td>\n",
       "      <td>1.822645</td>\n",
       "      <td>6.794249</td>\n",
       "      <td>1.214471</td>\n",
       "      <td>0.912614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>{'xgb__learning_rate': 0.1, 'xgb__n_estimators...</td>\n",
       "      <td>{\"xgb__learning_rate\": 0.1, \"xgb__n_estimators...</td>\n",
       "      <td>0.865814</td>\n",
       "      <td>2</td>\n",
       "      <td>2.511070</td>\n",
       "      <td>12.671853</td>\n",
       "      <td>2.009729</td>\n",
       "      <td>0.864809</td>\n",
       "      <td>0.874405</td>\n",
       "      <td>1</td>\n",
       "      <td>1.779893</td>\n",
       "      <td>7.453450</td>\n",
       "      <td>1.030370</td>\n",
       "      <td>0.874393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>{'xgb__learning_rate': 0.1, 'xgb__n_estimators...</td>\n",
       "      <td>{\"xgb__learning_rate\": 0.1, \"xgb__n_estimators...</td>\n",
       "      <td>0.917257</td>\n",
       "      <td>3</td>\n",
       "      <td>2.157178</td>\n",
       "      <td>8.057555</td>\n",
       "      <td>1.589156</td>\n",
       "      <td>0.917002</td>\n",
       "      <td>0.869113</td>\n",
       "      <td>2</td>\n",
       "      <td>2.693119</td>\n",
       "      <td>12.271260</td>\n",
       "      <td>2.454405</td>\n",
       "      <td>0.869083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>{'xgb__learning_rate': 0.1, 'xgb__n_estimators...</td>\n",
       "      <td>{\"xgb__learning_rate\": 0.1, \"xgb__n_estimators...</td>\n",
       "      <td>0.796827</td>\n",
       "      <td>4</td>\n",
       "      <td>2.660182</td>\n",
       "      <td>20.315691</td>\n",
       "      <td>1.513907</td>\n",
       "      <td>0.796801</td>\n",
       "      <td>0.918609</td>\n",
       "      <td>3</td>\n",
       "      <td>2.091659</td>\n",
       "      <td>7.903636</td>\n",
       "      <td>1.434607</td>\n",
       "      <td>0.918587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>{'xgb__learning_rate': 0.1, 'xgb__n_estimators...</td>\n",
       "      <td>{\"xgb__learning_rate\": 0.1, \"xgb__n_estimators...</td>\n",
       "      <td>0.870987</td>\n",
       "      <td>5</td>\n",
       "      <td>2.297164</td>\n",
       "      <td>14.350152</td>\n",
       "      <td>1.608324</td>\n",
       "      <td>0.854788</td>\n",
       "      <td>0.735580</td>\n",
       "      <td>4</td>\n",
       "      <td>2.908079</td>\n",
       "      <td>26.731201</td>\n",
       "      <td>1.444008</td>\n",
       "      <td>0.732632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>{'xgb__learning_rate': 0.1, 'xgb__n_estimators...</td>\n",
       "      <td>{\"xgb__learning_rate\": 0.1, \"xgb__n_estimators...</td>\n",
       "      <td>0.920622</td>\n",
       "      <td>6</td>\n",
       "      <td>2.289658</td>\n",
       "      <td>7.925646</td>\n",
       "      <td>1.829349</td>\n",
       "      <td>0.919489</td>\n",
       "      <td>0.866730</td>\n",
       "      <td>5</td>\n",
       "      <td>2.308649</td>\n",
       "      <td>14.544421</td>\n",
       "      <td>1.472402</td>\n",
       "      <td>0.852823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>{'xgb__learning_rate': 0.1, 'xgb__n_estimators...</td>\n",
       "      <td>{\"xgb__learning_rate\": 0.1, \"xgb__n_estimators...</td>\n",
       "      <td>0.837550</td>\n",
       "      <td>7</td>\n",
       "      <td>1.758184</td>\n",
       "      <td>9.445142</td>\n",
       "      <td>1.066853</td>\n",
       "      <td>0.837429</td>\n",
       "      <td>0.910525</td>\n",
       "      <td>6</td>\n",
       "      <td>2.302242</td>\n",
       "      <td>8.897191</td>\n",
       "      <td>1.844307</td>\n",
       "      <td>0.909620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>{'xgb__learning_rate': 0.1, 'xgb__n_estimators...</td>\n",
       "      <td>{\"xgb__learning_rate\": 0.1, \"xgb__n_estimators...</td>\n",
       "      <td>0.952020</td>\n",
       "      <td>8</td>\n",
       "      <td>1.740860</td>\n",
       "      <td>4.409847</td>\n",
       "      <td>1.420570</td>\n",
       "      <td>0.951984</td>\n",
       "      <td>0.785040</td>\n",
       "      <td>7</td>\n",
       "      <td>1.876944</td>\n",
       "      <td>12.508143</td>\n",
       "      <td>1.066783</td>\n",
       "      <td>0.784708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>{'xgb__learning_rate': 0.1, 'xgb__n_estimators...</td>\n",
       "      <td>{\"xgb__learning_rate\": 0.1, \"xgb__n_estimators...</td>\n",
       "      <td>0.926487</td>\n",
       "      <td>9</td>\n",
       "      <td>1.618849</td>\n",
       "      <td>4.282926</td>\n",
       "      <td>1.297615</td>\n",
       "      <td>0.926470</td>\n",
       "      <td>0.953309</td>\n",
       "      <td>8</td>\n",
       "      <td>1.675114</td>\n",
       "      <td>4.291643</td>\n",
       "      <td>1.459321</td>\n",
       "      <td>0.953271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>{'xgb__learning_rate': 0.1, 'xgb__n_estimators...</td>\n",
       "      <td>{\"xgb__learning_rate\": 0.1, \"xgb__n_estimators...</td>\n",
       "      <td>0.916742</td>\n",
       "      <td>0</td>\n",
       "      <td>1.847324</td>\n",
       "      <td>6.488884</td>\n",
       "      <td>1.350085</td>\n",
       "      <td>0.916541</td>\n",
       "      <td>0.921541</td>\n",
       "      <td>9</td>\n",
       "      <td>1.653786</td>\n",
       "      <td>4.585898</td>\n",
       "      <td>1.339002</td>\n",
       "      <td>0.921269</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      hyperparameters  \\\n",
       "50  {'xgb__learning_rate': 0.1, 'xgb__n_estimators...   \n",
       "51  {'xgb__learning_rate': 0.1, 'xgb__n_estimators...   \n",
       "52  {'xgb__learning_rate': 0.1, 'xgb__n_estimators...   \n",
       "53  {'xgb__learning_rate': 0.1, 'xgb__n_estimators...   \n",
       "54  {'xgb__learning_rate': 0.1, 'xgb__n_estimators...   \n",
       "55  {'xgb__learning_rate': 0.1, 'xgb__n_estimators...   \n",
       "56  {'xgb__learning_rate': 0.1, 'xgb__n_estimators...   \n",
       "57  {'xgb__learning_rate': 0.1, 'xgb__n_estimators...   \n",
       "58  {'xgb__learning_rate': 0.1, 'xgb__n_estimators...   \n",
       "59  {'xgb__learning_rate': 0.1, 'xgb__n_estimators...   \n",
       "\n",
       "                                  hyperparameters_str  \\\n",
       "50  {\"xgb__learning_rate\": 0.1, \"xgb__n_estimators...   \n",
       "51  {\"xgb__learning_rate\": 0.1, \"xgb__n_estimators...   \n",
       "52  {\"xgb__learning_rate\": 0.1, \"xgb__n_estimators...   \n",
       "53  {\"xgb__learning_rate\": 0.1, \"xgb__n_estimators...   \n",
       "54  {\"xgb__learning_rate\": 0.1, \"xgb__n_estimators...   \n",
       "55  {\"xgb__learning_rate\": 0.1, \"xgb__n_estimators...   \n",
       "56  {\"xgb__learning_rate\": 0.1, \"xgb__n_estimators...   \n",
       "57  {\"xgb__learning_rate\": 0.1, \"xgb__n_estimators...   \n",
       "58  {\"xgb__learning_rate\": 0.1, \"xgb__n_estimators...   \n",
       "59  {\"xgb__learning_rate\": 0.1, \"xgb__n_estimators...   \n",
       "\n",
       "    test_explained_variance  test_fold  test_mean_absolute_error  \\\n",
       "50                 0.874402          1                  1.753658   \n",
       "51                 0.865814          2                  2.511070   \n",
       "52                 0.917257          3                  2.157178   \n",
       "53                 0.796827          4                  2.660182   \n",
       "54                 0.870987          5                  2.297164   \n",
       "55                 0.920622          6                  2.289658   \n",
       "56                 0.837550          7                  1.758184   \n",
       "57                 0.952020          8                  1.740860   \n",
       "58                 0.926487          9                  1.618849   \n",
       "59                 0.916742          0                  1.847324   \n",
       "\n",
       "    test_mean_squared_error  test_median_absolute_error   test_r2  \\\n",
       "50                 7.543395                    1.297095  0.872877   \n",
       "51                12.671853                    2.009729  0.864809   \n",
       "52                 8.057555                    1.589156  0.917002   \n",
       "53                20.315691                    1.513907  0.796801   \n",
       "54                14.350152                    1.608324  0.854788   \n",
       "55                 7.925646                    1.829349  0.919489   \n",
       "56                 9.445142                    1.066853  0.837429   \n",
       "57                 4.409847                    1.420570  0.951984   \n",
       "58                 4.282926                    1.297615  0.926470   \n",
       "59                 6.488884                    1.350085  0.916541   \n",
       "\n",
       "    validation_explained_variance  validation_fold  \\\n",
       "50                       0.912643                0   \n",
       "51                       0.874405                1   \n",
       "52                       0.869113                2   \n",
       "53                       0.918609                3   \n",
       "54                       0.735580                4   \n",
       "55                       0.866730                5   \n",
       "56                       0.910525                6   \n",
       "57                       0.785040                7   \n",
       "58                       0.953309                8   \n",
       "59                       0.921541                9   \n",
       "\n",
       "    validation_mean_absolute_error  validation_mean_squared_error  \\\n",
       "50                        1.822645                       6.794249   \n",
       "51                        1.779893                       7.453450   \n",
       "52                        2.693119                      12.271260   \n",
       "53                        2.091659                       7.903636   \n",
       "54                        2.908079                      26.731201   \n",
       "55                        2.308649                      14.544421   \n",
       "56                        2.302242                       8.897191   \n",
       "57                        1.876944                      12.508143   \n",
       "58                        1.675114                       4.291643   \n",
       "59                        1.653786                       4.585898   \n",
       "\n",
       "    validation_median_absolute_error  validation_r2  \n",
       "50                          1.214471       0.912614  \n",
       "51                          1.030370       0.874393  \n",
       "52                          2.454405       0.869083  \n",
       "53                          1.434607       0.918587  \n",
       "54                          1.444008       0.732632  \n",
       "55                          1.472402       0.852823  \n",
       "56                          1.844307       0.909620  \n",
       "57                          1.066783       0.784708  \n",
       "58                          1.459321       0.953271  \n",
       "59                          1.339002       0.921269  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results[m_val_best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go back and select the predictions for the best hyperparameters\n",
    "best_res = [\n",
    "    res for res in all_res\n",
    "        if res.hyperparameters_str == val_best\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(best_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20.654224, 18.821327, 21.639122, 16.754532, 15.321813, 20.07061 ,\n",
       "       16.127214, 20.023327, 22.61253 , 41.999878, 23.750202, 17.175694,\n",
       "       14.028635, 13.158178, 18.151676, 17.758194, 21.566748, 27.407398,\n",
       "       38.46888 , 25.53746 , 24.041073, 21.12991 , 32.521378, 25.545212,\n",
       "       35.316124, 23.70998 , 31.244513, 47.158398, 24.47955 , 23.735378,\n",
       "       26.30453 , 18.260584, 20.323818, 20.774757, 23.592388, 13.737642,\n",
       "       11.497095,  8.396353, 11.563587, 13.739315, 14.433248, 16.893173,\n",
       "        9.005595, 18.918177, 16.17804 , 19.562008, 20.8405  , 19.49091 ,\n",
       "       25.620035, 19.670761, 23.72944 ], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_res[0].predictions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15. , 18.2, 23.1, 19.6, 14.8, 20. , 14.4, 19.7, 22.9, 43.8, 22.8,\n",
       "       17.4, 15.6, 13.8, 15.6, 13.1, 17.4, 29.6, 33.3, 22.6, 24.4, 21.5,\n",
       "       31.5, 25.1, 31.5, 23.7, 32. , 45.4, 22.1, 23.1, 24.5, 18.2, 21.7,\n",
       "       22.6, 25. , 13.3, 10.2,  7.4, 11.5, 12.5, 27.5, 15. ,  7. , 20.8,\n",
       "       16.4, 19.5, 19.9, 19.9, 29.8, 17.5, 23.9])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_res[0].true_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_groups['test_mean_absolute_error'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###\n",
    "# Extract masks for the training, validation, and testing\n",
    "#    sets.\n",
    "###\n",
    "splits = ml_utils.get_train_val_test_splits(\n",
    "    df,\n",
    "    validation_splits=validation_set,\n",
    "    test_splits=test_set,\n",
    "    split_field='fold'\n",
    ")\n",
    "\n",
    "###\n",
    "# Create the data matrices necessary for the various\n",
    "#    sklearn operations we will perform later.\n",
    "###\n",
    "\n",
    "# we do not want to use metadata fields\n",
    "fields_to_ignore = [\n",
    "    'fold'\n",
    "]\n",
    "\n",
    "fold_data = ml_utils.get_fold_data(\n",
    "    df,\n",
    "    target_field='target',\n",
    "    m_train=splits.training,\n",
    "    m_test=splits.test,\n",
    "    m_validation=splits.validation,\n",
    "    fields_to_ignore=fields_to_ignore\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Based on the template of our estimator pipeline template\n",
    "# and hyperparameters, create a concrete estimator with the\n",
    "# specified hyperparameters.\n",
    "###\n",
    "estimator = sklearn.clone(estimator_template)\n",
    "estimator = estimator.set_params(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Transform the target variable by taking the log(1+y).\n",
    "#\n",
    "# This operation may not help in all domains.\n",
    "###\n",
    "y_train = np.log1p(fold_data.y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Fit the estimator on the training set\n",
    "###\n",
    "estimator_fit = estimator.fit(fold_data.X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Use the fit estimator to make predictions on *both* the\n",
    "# validation and testing set. We will use both of these later.\n",
    "###\n",
    "y_pred = estimator_fit.predict(fold_data.X_test)\n",
    "y_val = estimator_fit.predict(fold_data.X_validation)\n",
    "\n",
    "# make sure to transform the predictions back to the original\n",
    "# scale using exp(y-1)\n",
    "y_pred = np.expm1(y_pred)\n",
    "y_val = np.expm1(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Collect various evaluation metrics of the trained model\n",
    "# on the validation and testing data.\n",
    "###\n",
    "metrics_val = ml_utils.collect_regression_metrics(fold_data.y_validation, y_val, prefix=\"val_\")\n",
    "metrics_test = ml_utils.collect_regression_metrics(fold_data.y_test, y_pred, prefix=\"test_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Construct a summary of the hyperparameters, validation\n",
    "# testing set, as well as the performance of the trained\n",
    "# model on those sets. We will use the performance on the\n",
    "# validation set across all folds in order to select the\n",
    "# optimal hyperparameters for evaluation on the test set.\n",
    "###\n",
    "\n",
    "ret = {\n",
    "    'val_set': validation_set,\n",
    "    'test_set': test_set,\n",
    "    'hyperparameter': str(hyperparameters),\n",
    "}\n",
    "\n",
    "ret.update(metrics_test)\n",
    "ret.update(metrics_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyllars",
   "language": "python",
   "name": "pyllars"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
